# Alertmanager Configuration for Sentinel AI Security Platform

global:
  # How long to wait before sending a notification again if it has already been sent
  resolve_timeout: 5m

  # SMTP configuration for email alerts
  smtp_smarthost: 'smtp.sendgrid.net:587'
  smtp_from: 'alerts@sentinel.ai'
  smtp_auth_username: 'apikey'
  smtp_auth_password: '${SMTP_PASSWORD}'

  # Slack configuration
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# Templates for notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routing tree
route:
  # Default receiver
  receiver: 'slack-notifications'

  # Group alerts by these labels
  group_by: ['alertname', 'severity', 'service']

  # How long to wait before sending initial notification
  group_wait: 30s

  # How long to wait before sending notification about new alerts in a group
  group_interval: 5m

  # How long to wait before resending a notification
  repeat_interval: 4h

  # Child routes for specific severity levels
  routes:
    # Critical alerts go to PagerDuty immediately
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 10s
      repeat_interval: 1h

    # Warning alerts go to Slack
    - match:
        severity: warning
      receiver: 'slack-notifications'
      group_wait: 1m
      repeat_interval: 4h

    # Security alerts get special handling
    - match:
        category: security
      receiver: 'security-team'
      group_wait: 10s
      repeat_interval: 30m

# Receivers configuration
receivers:
  # Slack notifications
  - name: 'slack-notifications'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#sentinel-alerts'
        send_resolved: true
        title: '{{ .Status | toUpper }}{{ if eq .Status "firing" }} :fire:{{ else }} :white_check_mark:{{ end }} [{{ .CommonLabels.severity | toUpper }}]'
        text: |
          *Alert:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Severity:* {{ .CommonLabels.severity }}
          {{ range .Alerts }}
          *Details:*
          {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
          {{ end }}
          {{ end }}
        icon_emoji: ':warning:'
        username: 'Sentinel Alerts'

  # PagerDuty for critical alerts
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        severity: critical
        description: '{{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          num_resolved: '{{ .Alerts.Resolved | len }}'

  # Security team alerts
  - name: 'security-team'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#security-incidents'
        send_resolved: true
        title: ':rotating_light: SECURITY ALERT: {{ .CommonLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Severity:* {{ .CommonLabels.severity }}
          {{ range .Alerts }}
          *Details:*
          {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
          {{ end }}
          {{ end }}
        icon_emoji: ':lock:'
        username: 'Security Bot'
    email_configs:
      - to: 'security@sentinel.ai'
        send_resolved: true
        html: '{{ template "email.default.html" . }}'

  # Null receiver for silencing
  - name: 'null'

# Inhibition rules to suppress alerts
inhibit_rules:
  # Inhibit warning alerts if critical alert is firing for the same service
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']

  # Inhibit all alerts if the cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: '.+'
    equal: ['cluster']
